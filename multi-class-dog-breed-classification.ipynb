{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-class Dog Breed Classification\nMulticlass Dog Breed classification model build with TensorFlow and TensorFlow Hub\n\n## 1. Problem\nGiven an image of a dog, asked to identify its breed.\n\n## 2. Data\nData used is taken from Stanford University Dogs Dataset http://vision.stanford.edu/aditya86/ImageNetDogs/\n\n## 3. Evaluation\nThe resultant model will be evaluated based on the Multi-class Log Loss.\n\n## 4. Features\n- Unstructured data (images)\n- 120 Categories (dog breeds)\n- Total of 20579 unique images.\n- Approximately 150 images per breed.","metadata":{"id":"40b6b05e-d9bb-4960-b20f-c99240feb05b"}},{"cell_type":"markdown","source":"## Note: \n- The data has been downloaded and extracted beforehand, hence why the procedure is not shown in the notebook. \n- ^^ Personally retrieved the data with `!wget` and then extracted it with the `tarfile` library.\n- It can be noted that one of the image in the dataset may be corrupted during extraction (as seen from the use of 20579 instead of 20580), \nso errors when processing/fitting the whole data might be occur.\n- A simple solution that was done was the use a `Binary Search` strategy to find the corrupted image and remove it from the data.","metadata":{"id":"8ca980a0-ac11-4cdb-8f2e-b5de65617dd0"}},{"cell_type":"markdown","source":"# First, We Need to Prepare Libraries","metadata":{"id":"21ad5370-2a04-483a-84b3-7e8409bfcb03"}},{"cell_type":"code","source":"#Import libraries\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import Image\nimport os\nprint(\"TF version:\", tf.__version__)\nprint(\"TF Hub version:\", hub.__version__)\n\n#Check whether theres GPU available\nprint(\"GPU\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"not available\")","metadata":{"id":"268f39ec-fd2f-40a6-a163-1e2dada7e781","outputId":"eaf857b4-9b97-4032-d6d9-879d920785bb","execution":{"iopub.status.busy":"2022-08-05T03:05:08.952151Z","iopub.execute_input":"2022-08-05T03:05:08.953137Z","iopub.status.idle":"2022-08-05T03:05:18.320989Z","shell.execute_reply.started":"2022-08-05T03:05:08.953016Z","shell.execute_reply":"2022-08-05T03:05:18.319093Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"TF version: 2.6.4\nTF Hub version: 0.12.0\nGPU available\n","output_type":"stream"},{"name":"stderr","text":"2022-08-05 03:05:18.129575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-05 03:05:18.289461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-05 03:05:18.295496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Second, We need to prepare the datasets\n## First we get the filenames and labels","metadata":{"id":"7fe4ad52-dcb7-4021-9d89-ec3c5c61d5fa"}},{"cell_type":"code","source":"labels = []\nfolder_name = []\nfilenames = []\n\nfor folders in os.listdir(\"../input/stanford-dogs-dataset/images/Images\"):\n    breed = \"\".join(folders.split(\"-\")[1:])\n    for fname in os.listdir(f\"../input/stanford-dogs-dataset/images/Images/{folders}\"):\n        if fname[-3:] == \"jpg\":\n            labels.append(breed)\n            folder_name.append(folders)\n            filenames.append(f\"../input/stanford-dogs-dataset/images/Images/{folders}/{fname}\")\n        else:\n            print(fname)\n\n# convert to np array for convinience\nlabels = np.array(labels)\nfilenames = np.array(filenames)\nlen(labels), len(filenames), len(folder_name), labels[0], filenames[0], folder_name[0]","metadata":{"id":"a211ca5b-19f5-41a0-af6c-442b4a84df45","outputId":"a3bf5d8f-ee40-4f9e-9659-c9f444bba544","execution":{"iopub.status.busy":"2022-08-05T03:12:29.056064Z","iopub.execute_input":"2022-08-05T03:12:29.056470Z","iopub.status.idle":"2022-08-05T03:12:29.218373Z","shell.execute_reply.started":"2022-08-05T03:12:29.056438Z","shell.execute_reply":"2022-08-05T03:12:29.216859Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(20580,\n 20580,\n 20580,\n 'otterhound',\n '../input/stanford-dogs-dataset/images/Images/n02091635-otterhound/n02091635_965.jpg',\n 'n02091635-otterhound')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Then we get the unique labels","metadata":{"id":"9967b681-b648-43dc-bc45-bb852904b0cb"}},{"cell_type":"code","source":"# Create unique array for both breed and folder & makes sure both are sorted in the same way\nunique_breeds = np.unique(labels)\nunique_folder = sorted(np.unique(folder_name), key=lambda x:\"\".join(x.split(\"-\")[1:]))\nlen(unique_breeds), unique_breeds[:10], len(unique_folder), unique_folder[:10]","metadata":{"id":"e09afb4a-a8f3-42d7-a40a-ce8b7b92a43b","outputId":"5ca14179-0a41-4498-9d95-68fe76e59511","execution":{"iopub.status.busy":"2022-08-05T03:15:58.853276Z","iopub.execute_input":"2022-08-05T03:15:58.853765Z","iopub.status.idle":"2022-08-05T03:15:58.885042Z","shell.execute_reply.started":"2022-08-05T03:15:58.853736Z","shell.execute_reply":"2022-08-05T03:15:58.883789Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(120,\n array(['Afghan_hound', 'African_hunting_dog', 'Airedale',\n        'American_Staffordshire_terrier', 'Appenzeller',\n        'Australian_terrier', 'Bedlington_terrier', 'Bernese_mountain_dog',\n        'Blenheim_spaniel', 'Border_collie'], dtype='<U30'),\n 120,\n ['n02088094-Afghan_hound',\n  'n02116738-African_hunting_dog',\n  'n02096051-Airedale',\n  'n02093428-American_Staffordshire_terrier',\n  'n02107908-Appenzeller',\n  'n02096294-Australian_terrier',\n  'n02093647-Bedlington_terrier',\n  'n02107683-Bernese_mountain_dog',\n  'n02086646-Blenheim_spaniel',\n  'n02106166-Border_collie'])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Next step is to preprocess our image data into a format which can be processed and trained by TensorFlow aka tensor batches using the `ImageDataGenerator`\nhttps://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator","metadata":{"id":"9f170a72-ad24-440f-b12c-d30b6f7aa3d2"}},{"cell_type":"code","source":"BATCH_SIZE = 32\nIMG_SIZE = 224\n\n# For base model training data\ntrain_data_generator = ImageDataGenerator(\n    validation_split=0.2,\n    rescale=1./255)\n\n# For augmented training data\naugmented_data_generator = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2,\n    rescale=1./255)","metadata":{"execution":{"iopub.status.busy":"2022-08-05T03:36:14.322229Z","iopub.execute_input":"2022-08-05T03:36:14.322614Z","iopub.status.idle":"2022-08-05T03:36:14.331735Z","shell.execute_reply.started":"2022-08-05T03:36:14.322585Z","shell.execute_reply":"2022-08-05T03:36:14.329238Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"print(\"TRAINING DATA\") # TRAIN DATA\ntrain_data = train_data_generator.flow_from_directory(\"../input/stanford-dogs-dataset/images/Images/\",\n                                                      batch_size=BATCH_SIZE,\n                                                      target_size=(IMG_SIZE, IMG_SIZE),\n                                                      classes=unique_folder,\n                                                      subset='training',\n                                                      shuffle=True,\n                                                      seed=42) #Use of seed so that same results can be engineered again \n\nprint(\"VALID DATA\") # VALID DATA\nvalid_data = train_data_generator.flow_from_directory(\"../input/stanford-dogs-dataset/images/Images/\",\n                                                      batch_size=BATCH_SIZE,\n                                                      target_size=(IMG_SIZE, IMG_SIZE),\n                                                      classes=unique_folder,\n                                                      subset='validation',\n                                                      shuffle=False,\n                                                      seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-08-05T03:27:17.550391Z","iopub.execute_input":"2022-08-05T03:27:17.550948Z","iopub.status.idle":"2022-08-05T03:27:19.216200Z","shell.execute_reply.started":"2022-08-05T03:27:17.550904Z","shell.execute_reply":"2022-08-05T03:27:19.214582Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"TRAINING DATA\nFound 16508 images belonging to 120 classes.\nVALID DATA\nFound 4072 images belonging to 120 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Building a Model !\n\nBefore we build a model, there are a few things we need to define:\n* The input shape (image shape in the form of Tensors)\n* The output shape (image labels in the form of Tensors)\n* URL of model we want to use Transfer Learning with from TensorFlow Hub\nhttps://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5\n\n^^ MobileNet Image Classification (Size = 224x224, Depth Multiplier = 1.3)","metadata":{"execution":{"iopub.execute_input":"2022-06-25T06:15:31.824930Z","iopub.status.busy":"2022-06-25T06:15:31.823717Z","iopub.status.idle":"2022-06-25T06:15:31.831293Z","shell.execute_reply":"2022-06-25T06:15:31.830236Z","shell.execute_reply.started":"2022-06-25T06:15:31.824893Z"},"id":"64bc0d33-cf08-4048-8351-09a346cd33bb"}},{"cell_type":"code","source":"#setup input shape into the model\nINPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] #batch, height, width, colour channel\n\n#setup output shape of our model\nOUTPUT_SHAPE = len(unique_breeds)\n\n#setup model URL from Tensorflow HUB\nMODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5\"","metadata":{"id":"581ee656-39bb-4a93-a751-a6d91e81c225","execution":{"iopub.status.busy":"2022-08-05T03:27:42.983974Z","iopub.execute_input":"2022-08-05T03:27:42.984363Z","iopub.status.idle":"2022-08-05T03:27:42.992115Z","shell.execute_reply.started":"2022-08-05T03:27:42.984335Z","shell.execute_reply":"2022-08-05T03:27:42.990587Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Now we've got our Inputs, Outputs, and Model ready...\nLets put them together into a Keras Deep Learning Model!\n\nKnowing this, let's create a function that:\n* takes input shape, output shape, and model as parameters\n* Defines the layers of KEras Model in a sequential fashion\n* Compiles the model (Says it should be evaluated and improved)\n* Builds model (Tell model the input shape it'll be getting)\n* Return the model\n\nAll the steps can be found in https://www.tensorflow.org/guide/keras/overview","metadata":{"id":"a640fbae-53b6-4f89-bb8a-8812511be8f7"}},{"cell_type":"code","source":"# Create a Function which builds a Keras Model\ndef create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):\n  print(f\"Building model with {MODEL_URL}\")\n\n  # Setup Model Layers\n  model = tf.keras.Sequential([\n    hub.KerasLayer(MODEL_URL), # Layer 1 (Input Layer)\n    tf.keras.layers.Dense(units=output_shape, \n                          activation=\"softmax\") # Layer 2 (Output Layer)       \n  ])\n\n  # Compile the model\n  model.compile(\n      loss=tf.keras.losses.CategoricalCrossentropy(),\n      optimizer=tf.keras.optimizers.Adam(),\n      metrics =[\"accuracy\"]\n  )\n\n  # Build Model\n  model.build(input_shape)\n\n  return model","metadata":{"id":"6128cc86-77d5-4d56-98ba-9f0ddae34ad2","execution":{"iopub.status.busy":"2022-08-05T03:27:51.882634Z","iopub.execute_input":"2022-08-05T03:27:51.883054Z","iopub.status.idle":"2022-08-05T03:27:51.892249Z","shell.execute_reply.started":"2022-08-05T03:27:51.883025Z","shell.execute_reply":"2022-08-05T03:27:51.890064Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model = create_model()\nmodel.summary()","metadata":{"id":"3c73ecf7-2f55-486e-9e19-a40222610c4b","outputId":"60972d51-1911-4e75-e8eb-f082c7c394e4","execution":{"iopub.status.busy":"2022-08-05T03:27:54.195444Z","iopub.execute_input":"2022-08-05T03:27:54.195873Z","iopub.status.idle":"2022-08-05T03:28:03.055961Z","shell.execute_reply.started":"2022-08-05T03:27:54.195827Z","shell.execute_reply":"2022-08-05T03:28:03.053483Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Building model with https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5\n","output_type":"stream"},{"name":"stderr","text":"2022-08-05 03:27:56.530192: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-08-05 03:27:56.530859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-05 03:27:56.536235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-05 03:27:56.541638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-05 03:27:59.487878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-05 03:27:59.492030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-05 03:27:59.496014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-05 03:27:59.499791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nkeras_layer (KerasLayer)     (None, 1001)              5432713   \n_________________________________________________________________\ndense (Dense)                (None, 120)               120240    \n=================================================================\nTotal params: 5,552,953\nTrainable params: 120,240\nNon-trainable params: 5,432,713\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Creating Callbacks\n\nCallbacks are helper functions a model can use during training to:\n* Save its progress\n* Check its progress\n* Stop Training early if it stops improving\n\nWe'll create 2 callbacks,\n- One is for TensorBoard, helps track our model progress\n- Second is for early stopping, prevents model from training too long (becoming overfitted)\n\n\n### TensorBoard Callback\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n\nTo setup the Tensorboard Callback, we need to:\n1. Load a TensorBoard notebook Extension\n2. Create a Tensorboard Callback which save logs to a directory and pass it to the model's `fit()` function.\n3. Visualize our models training logs with the `%tensorboard` magic function (we'll do this after model training)","metadata":{"id":"8a0eabc7-3972-4acd-9111-7a7820ba1a9b"}},{"cell_type":"code","source":"# Load TensorBoard Notebook Extension\n%load_ext tensorboard\n\nimport datetime\n\n# Create TensorBoard Callback Function\ndef create_tensorboard_callback():\n  # Create a log directory for storing TensorBoard logs\n  logdir = os.path.join(\"Logs\",\n                        datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") # Tracks time everytime we run experiment\n                        )\n  return tf.keras.callbacks.TensorBoard(logdir)","metadata":{"id":"e8c534f3-9cb9-4dbc-8964-269dc9e28f60","outputId":"b7a55bd6-44a9-4224-9c6d-be3e87ed6a04","execution":{"iopub.status.busy":"2022-08-04T13:28:18.761982Z","iopub.execute_input":"2022-08-04T13:28:18.762380Z","iopub.status.idle":"2022-08-04T13:28:18.781037Z","shell.execute_reply.started":"2022-08-04T13:28:18.762349Z","shell.execute_reply":"2022-08-04T13:28:18.779916Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"### Early Stopping Callback\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n\nHelps model from overfitting by stopping training if a certain evaluation metric stops improving","metadata":{"id":"cf6acb5e-99ce-4e75-be85-c26404600c72"}},{"cell_type":"code","source":"# Create early stopping callback\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                                  patience=10)","metadata":{"id":"c06f18ee-515e-4849-9f64-1777039f0931","execution":{"iopub.status.busy":"2022-08-04T13:28:32.428442Z","iopub.execute_input":"2022-08-04T13:28:32.428811Z","iopub.status.idle":"2022-08-04T13:28:32.433849Z","shell.execute_reply.started":"2022-08-04T13:28:32.428779Z","shell.execute_reply":"2022-08-04T13:28:32.432972Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### Model Checkpoint Callback\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n\nCallback to save the Keras model or model weights at some frequency.","metadata":{}},{"cell_type":"code","source":"def create_model_checkpoint(suffix=None):\n    modeldir = os.path.join(\"Models\",\n                          datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M_%s\"))\n    \n    model_path = modeldir + \"-\" + suffix\n    return tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss',save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T13:28:37.136521Z","iopub.execute_input":"2022-08-04T13:28:37.136894Z","iopub.status.idle":"2022-08-04T13:28:37.143208Z","shell.execute_reply.started":"2022-08-04T13:28:37.136859Z","shell.execute_reply":"2022-08-04T13:28:37.141764Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"Let's create a function which trains a model.\n\n* Create a model using `create_model`\n* Setup a TensorBoard callback using `create_tensorboard_callback()`\n* Call the `fit()` function on our model passing it the training data, validation data, num of epochs to train for (`NUM_EPOCHS`), and callbacks\n* Return Model","metadata":{"id":"e3c954dc-0287-4cac-a51a-e16b4983a2a0"}},{"cell_type":"code","source":"NUM_EPOCHS = 100\n\ndef train_model(train_data, valid=False, val_data=None):\n    \"\"\"\n    Trains a given model and returns the trained version.\n    \"\"\"\n\n    # Create model\n    model = create_model()\n\n    # Create new TensorBoard session everytime we train the model\n    tensorboard = create_tensorboard_callback()\n    checkpoint = create_model_checkpoint(\"trained_1000\")\n    # Fit the model\n    if valid:\n        # Create early stopping callback\n        early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                                      patience=10)\n        model.fit(x=train_data,\n                epochs=NUM_EPOCHS,\n                validation_data=val_data,\n                validation_freq=1,\n                callbacks=[tensorboard, early_stopping, checkpoint]\n                )\n    else:\n        # Create early stopping callback\n        early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\",\n                                                  patience=10)\n        model.fit(x=train_data,\n                epochs=NUM_EPOCHS,\n                callbacks=[tensorboard, early_stopping]\n                )\n  \n    # Return the fitted model\n    return model","metadata":{"id":"8862cb69-af62-4759-8c26-d7796c77f3b0","execution":{"iopub.status.busy":"2022-08-04T13:29:04.045401Z","iopub.execute_input":"2022-08-04T13:29:04.046015Z","iopub.status.idle":"2022-08-04T13:29:04.056182Z","shell.execute_reply.started":"2022-08-04T13:29:04.045976Z","shell.execute_reply":"2022-08-04T13:29:04.053145Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"# With all prepared, it is time to train the model with the \n## To start off, lets try training 1000 data first","metadata":{"id":"c491fb98-cd7e-4831-ad10-6e836cd6fdc7"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Fit the model to data\nmodel = train_model(train_data, valid=True, val_data=valid_data)","metadata":{"id":"5ecbc064-7ce0-42fd-bd21-c57d7de8c5e8","outputId":"c1eb9c43-0cc8-47b9-c037-9c46fc38aa6c","execution":{"iopub.status.busy":"2022-08-04T13:31:18.568484Z","iopub.execute_input":"2022-08-04T13:31:18.569450Z","iopub.status.idle":"2022-08-04T13:52:40.491876Z","shell.execute_reply.started":"2022-08-04T13:31:18.569401Z","shell.execute_reply":"2022-08-04T13:52:40.490973Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Building model with https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5\n","output_type":"stream"},{"name":"stderr","text":"2022-08-04 13:31:20.738317: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n2022-08-04 13:31:20.738367: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n2022-08-04 13:31:20.740903: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n2022-08-04 13:31:21.028920: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n2022-08-04 13:31:21.029113: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n2022-08-04 13:31:21.405382: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"2022-08-04 13:31:25.865868: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"  1/516 [..............................] - ETA: 1:26:52 - loss: 5.9735 - accuracy: 0.0000e+00","output_type":"stream"},{"name":"stderr","text":"2022-08-04 13:31:31.877452: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n2022-08-04 13:31:31.877512: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n","output_type":"stream"},{"name":"stdout","text":"  2/516 [..............................] - ETA: 6:20 - loss: 5.8581 - accuracy: 0.0000e+00   ","output_type":"stream"},{"name":"stderr","text":"2022-08-04 13:31:32.306384: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n2022-08-04 13:31:32.307179: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n2022-08-04 13:31:32.455515: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 284 callback api events and 281 activity events. \n2022-08-04 13:31:32.473452: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n2022-08-04 13:31:32.495269: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: Logs/20220804-133120/train/plugins/profile/2022_08_04_13_31_32\n\n2022-08-04 13:31:32.505864: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to Logs/20220804-133120/train/plugins/profile/2022_08_04_13_31_32/3cc5f13990e9.trace.json.gz\n","output_type":"stream"},{"name":"stdout","text":"  3/516 [..............................] - ETA: 4:39 - loss: 5.8592 - accuracy: 0.0000e+00","output_type":"stream"},{"name":"stderr","text":"2022-08-04 13:31:32.555152: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: Logs/20220804-133120/train/plugins/profile/2022_08_04_13_31_32\n\n2022-08-04 13:31:32.563958: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to Logs/20220804-133120/train/plugins/profile/2022_08_04_13_31_32/3cc5f13990e9.memory_profile.json.gz\n2022-08-04 13:31:32.565781: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: Logs/20220804-133120/train/plugins/profile/2022_08_04_13_31_32\nDumped tool data for xplane.pb to Logs/20220804-133120/train/plugins/profile/2022_08_04_13_31_32/3cc5f13990e9.xplane.pb\nDumped tool data for overview_page.pb to Logs/20220804-133120/train/plugins/profile/2022_08_04_13_31_32/3cc5f13990e9.overview_page.pb\nDumped tool data for input_pipeline.pb to Logs/20220804-133120/train/plugins/profile/2022_08_04_13_31_32/3cc5f13990e9.input_pipeline.pb\nDumped tool data for tensorflow_stats.pb to Logs/20220804-133120/train/plugins/profile/2022_08_04_13_31_32/3cc5f13990e9.tensorflow_stats.pb\nDumped tool data for kernel_stats.pb to Logs/20220804-133120/train/plugins/profile/2022_08_04_13_31_32/3cc5f13990e9.kernel_stats.pb\n\n","output_type":"stream"},{"name":"stdout","text":"516/516 [==============================] - 219s 406ms/step - loss: 1.1404 - accuracy: 0.7051 - val_loss: 0.7075 - val_accuracy: 0.7876\n","output_type":"stream"},{"name":"stderr","text":"2022-08-04 13:35:02.606461: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/100\n516/516 [==============================] - 96s 186ms/step - loss: 0.4305 - accuracy: 0.8664 - val_loss: 0.6881 - val_accuracy: 0.7974\nEpoch 3/100\n516/516 [==============================] - 97s 189ms/step - loss: 0.2805 - accuracy: 0.9144 - val_loss: 0.7082 - val_accuracy: 0.7959\nEpoch 4/100\n516/516 [==============================] - 97s 188ms/step - loss: 0.1984 - accuracy: 0.9420 - val_loss: 0.7426 - val_accuracy: 0.7935\nEpoch 5/100\n516/516 [==============================] - 99s 191ms/step - loss: 0.1460 - accuracy: 0.9589 - val_loss: 0.7740 - val_accuracy: 0.7859\nEpoch 6/100\n516/516 [==============================] - 96s 187ms/step - loss: 0.1137 - accuracy: 0.9696 - val_loss: 0.7937 - val_accuracy: 0.7935\nEpoch 7/100\n516/516 [==============================] - 94s 182ms/step - loss: 0.0894 - accuracy: 0.9778 - val_loss: 0.8242 - val_accuracy: 0.7851\nEpoch 8/100\n516/516 [==============================] - 94s 182ms/step - loss: 0.0721 - accuracy: 0.9837 - val_loss: 0.8636 - val_accuracy: 0.7856\nEpoch 9/100\n516/516 [==============================] - 93s 180ms/step - loss: 0.0605 - accuracy: 0.9869 - val_loss: 0.8718 - val_accuracy: 0.7917\nEpoch 10/100\n516/516 [==============================] - 93s 180ms/step - loss: 0.0530 - accuracy: 0.9893 - val_loss: 0.8960 - val_accuracy: 0.7915\nEpoch 11/100\n516/516 [==============================] - 96s 185ms/step - loss: 0.0456 - accuracy: 0.9909 - val_loss: 0.9462 - val_accuracy: 0.7849\nEpoch 12/100\n516/516 [==============================] - 95s 184ms/step - loss: 0.0430 - accuracy: 0.9915 - val_loss: 0.9710 - val_accuracy: 0.7834\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Checking the TensorBoard Logs\n\nthe Tensor board function `%tensorboard` will access the logs we created and visualize its content","metadata":{"id":"9462eb37-61c8-4f7f-a975-ff6e6128da4b"}},{"cell_type":"code","source":"%tensorboard --logdir Logs","metadata":{"id":"f1c372f9-1be8-40b3-9d91-5b99f8e63d98","outputId":"7cd722cb-e032-424d-9f92-1d0caa935395","execution":{"iopub.status.busy":"2022-08-03T16:13:06.388619Z","iopub.execute_input":"2022-08-03T16:13:06.389345Z","iopub.status.idle":"2022-08-03T16:13:10.995548Z","shell.execute_reply.started":"2022-08-03T16:13:06.389302Z","shell.execute_reply":"2022-08-03T16:13:10.994239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make a prediction using the trained model and evaluate VISUALLY","metadata":{"id":"cc5a0362-f961-45c9-8c69-04d1679c80a3"}},{"cell_type":"code","source":"# Make Predictions on validation data (not trained on)\n\npreds = model.predict(valid_data, verbose=1)\npreds","metadata":{"id":"1082ce24-dde4-409f-a0c4-83ce82bcf38b","outputId":"679b2a14-4168-42a9-9b94-1de549b5089d","execution":{"iopub.status.busy":"2022-08-04T14:13:35.341077Z","iopub.execute_input":"2022-08-04T14:13:35.341447Z","iopub.status.idle":"2022-08-04T14:13:53.726608Z","shell.execute_reply.started":"2022-08-04T14:13:35.341414Z","shell.execute_reply":"2022-08-04T14:13:53.725446Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"128/128 [==============================] - 18s 137ms/step\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"array([[9.96509135e-01, 1.70906386e-10, 7.27959193e-10, ...,\n        2.85373244e-06, 7.75010221e-07, 4.50318938e-09],\n       [9.85191464e-01, 2.31939423e-09, 1.94032193e-10, ...,\n        1.96913987e-08, 1.33860524e-07, 1.23235022e-08],\n       [9.99999523e-01, 3.58226686e-13, 6.26219979e-11, ...,\n        6.51419418e-10, 1.07026763e-15, 1.17281282e-10],\n       ...,\n       [6.83417775e-12, 7.36521399e-10, 4.54512782e-11, ...,\n        3.72738526e-07, 9.02669784e-03, 9.90970135e-01],\n       [1.40641275e-11, 5.23841495e-13, 2.14693083e-10, ...,\n        1.83153509e-06, 5.02523285e-08, 9.99996781e-01],\n       [2.13505429e-11, 2.97548292e-12, 3.47029129e-07, ...,\n        3.17837803e-05, 2.80817534e-04, 9.99481738e-01]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"train_data[0][1][1]","metadata":{"execution":{"iopub.status.busy":"2022-08-04T14:16:34.171090Z","iopub.execute_input":"2022-08-04T14:16:34.171466Z","iopub.status.idle":"2022-08-04T14:16:34.299646Z","shell.execute_reply.started":"2022-08-04T14:16:34.171433Z","shell.execute_reply":"2022-08-04T14:16:34.298718Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"0.0"},"metadata":{}}]},{"cell_type":"code","source":"# Lets make a function that convert the array of numbers into a label prediction...\ndef get_pred_label(prediction_probabilities):\n  \"\"\"\n  Turns an array of prediction probabilities into a label.\n  \"\"\"\n  return unique_breeds[np.argmax(prediction_probabilities)]\n\n# get_pred_label(pred[0])","metadata":{"id":"0d3c9b85-a261-4cc9-ac46-f134113fdd56","execution":{"iopub.status.busy":"2022-08-03T16:18:14.316391Z","iopub.execute_input":"2022-08-03T16:18:14.316766Z","iopub.status.idle":"2022-08-03T16:18:14.321941Z","shell.execute_reply.started":"2022-08-03T16:18:14.316732Z","shell.execute_reply":"2022-08-03T16:18:14.320958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to unbatch datasets\ndef unbatchify(data):\n  \"\"\"\n  Takes batched dataset of (image, labels) Tensors and \n  returns separate arrays of images and labels.\n  \"\"\"\n\n  images_ = []\n  labels_ = []\n  # Loop through unbatched data and unbatch em\n  for image, label in data.unbatch().as_numpy_iterator():\n    images_.append(image)\n    labels_.append(get_pred_label(label))\n\n  return images_, labels_\n\n# unbatched_image, unbatched_label = unbatchify()\n# unbatched_image[0], unbatched_label[0]","metadata":{"id":"9d212ee5-ec00-44e2-9ded-31bc5ff8da07","execution":{"iopub.status.busy":"2022-08-03T16:18:19.599662Z","iopub.execute_input":"2022-08-03T16:18:19.600394Z","iopub.status.idle":"2022-08-03T16:18:19.606234Z","shell.execute_reply.started":"2022-08-03T16:18:19.600358Z","shell.execute_reply":"2022-08-03T16:18:19.605097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Personal function to check accuracy of prediction\ndef preds_check(index=0,preds=preds, verbose=False):\n  if verbose:\n    print(preds[index])\n  print(f\"Max value (probability of prediction) : {np.max(preds[index])}\")\n  print(f\"Sum : {np.sum(preds[index])}\")\n  print(f\"Max Index: {np.argmax(preds[index])}\")\n  print(f\"Predicted label: {get_pred_label(preds[index])}\")\n  print(f\"Actual label: {unbatched_label[index]}\")","metadata":{"execution":{"iopub.execute_input":"2022-06-25T08:11:00.182176Z","iopub.status.busy":"2022-06-25T08:11:00.181860Z","iopub.status.idle":"2022-06-25T08:11:00.187146Z","shell.execute_reply":"2022-06-25T08:11:00.186422Z","shell.execute_reply.started":"2022-06-25T08:11:00.182148Z"},"id":"f5e2fbc7-ab35-4e34-9799-4efb84afc4fe","outputId":"2da8a39c-e52d-4761-bd7a-69413163f702"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_check(0)","metadata":{"execution":{"iopub.execute_input":"2022-06-25T08:11:12.789086Z","iopub.status.busy":"2022-06-25T08:11:12.788384Z","iopub.status.idle":"2022-06-25T08:11:12.794471Z","shell.execute_reply":"2022-06-25T08:11:12.793557Z","shell.execute_reply.started":"2022-06-25T08:11:12.789045Z"},"id":"7ec20a0f-a87c-4743-9aee-ac3cb8d01afa","outputId":"78a030ad-0ce1-46b3-c625-579069c11737"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_pred(prediction_probabilities, labels, images, n=1):\n  \"\"\"\n  View the prediction, ground truth and image for sample n\n  \"\"\"\n  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]\n\n  # Get pred label\n  pred_label = get_pred_label(pred_prob)\n\n  # Plot image\n  plt.imshow(image)\n  plt.xticks([])\n  plt.yticks([])\n\n  # Change color if right or wrong\n  if pred_label == true_label:\n    color = \"green\"\n  else:\n    color = \"red\"\n  # Change plot title to predicted, prob predicted, truth label\n  plt.title(\"{} / {:2.2f}% / {}\".format(pred_label, \n                                        np.max(pred_prob)*100,\n                                        true_label),\n            color=color)\n\n    \ndef plot_pred_conf(prediction_probabilities, labels, n=1):\n  \"\"\"\n  Plots the top 10 highest prediction confidences along with truth label for sample n.\n  \"\"\"\n  pred_prob, true_label = prediction_probabilities[n], get_pred_label(labels[n])\n\n  # Get predicted label\n  pred_label = get_pred_label(pred_prob)\n\n  # Find top 10 prediction confidence indexes\n  top_10_pred_index = pred_prob.argsort()[-10:][::-1]\n  # Find top 10 prediction confidence values\n  top_10_pred_value = pred_prob[top_10_pred_index]\n  # Find top 10 prediction labels\n  top_10_pred_labels = unique_breeds[top_10_pred_index]\n\n  # Plot\n  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),\n                     top_10_pred_value,\n                     color=\"grey\")\n  plt.xticks(np.arange(len(top_10_pred_labels)),\n             labels=top_10_pred_labels,\n             rotation=\"vertical\")\n\n  # Change color\n  if np.isin(true_label, top_10_pred_labels).any():\n    print(\"Truth Label = \", true_label)\n    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color(\"green\")","metadata":{"execution":{"iopub.execute_input":"2022-06-25T08:11:55.845248Z","iopub.status.busy":"2022-06-25T08:11:55.845003Z","iopub.status.idle":"2022-06-25T08:11:55.853396Z","shell.execute_reply":"2022-06-25T08:11:55.852684Z","shell.execute_reply.started":"2022-06-25T08:11:55.845223Z"},"id":"2c46d42e-24a8-489d-905f-0bce6fd2cdb6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pred(preds,unbatched_label,unbatched_image, 4)","metadata":{"execution":{"iopub.execute_input":"2022-06-25T08:12:13.445414Z","iopub.status.busy":"2022-06-25T08:12:13.445150Z","iopub.status.idle":"2022-06-25T08:12:13.528763Z","shell.execute_reply":"2022-06-25T08:12:13.528117Z","shell.execute_reply.started":"2022-06-25T08:12:13.445388Z"},"id":"65608ffb-5a59-4184-a408-b0b3dcf27920","outputId":"9a7975c9-85ff-4d23-c42a-b6712e0cbfc9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pred_conf(preds, unbatched_label, 4)","metadata":{"execution":{"iopub.execute_input":"2022-06-25T08:12:25.909376Z","iopub.status.busy":"2022-06-25T08:12:25.908580Z","iopub.status.idle":"2022-06-25T08:12:26.051546Z","shell.execute_reply":"2022-06-25T08:12:26.050767Z","shell.execute_reply.started":"2022-06-25T08:12:25.909347Z"},"id":"2298f0d2-1f65-456c-86dd-a80c344ed855","outputId":"e96aa789-b86a-41fe-d58d-d083d8e4a041"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving and reloading a trained model","metadata":{"id":"e41cdb94-c19b-48c1-a9b0-13505155adea"}},{"cell_type":"code","source":"# Create a function to save model\ndef save_model(model, suffix=None):\n  \"\"\"\n  Saves a given model in a models directory and appends a suffix(string)\n  \"\"\"\n\n  # Create model directory pathname. with current time\n  modeldir = os.path.join(\"Models\",\n                          datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M_%s\"))\n  model_path = modeldir + \"-\" + suffix + \".h5\"\n  print(f\"Saving model to: {model_path}...\")\n  model.save(model_path)\n  return model_path\n\n# Create a function to load model\ndef load_model(model_path):\n  \"\"\"\n  Loads a saved model from a specified path\n  \"\"\"\n  print(f\"Loading saved model from: {model_path}\")\n  model = tf.keras.models.load_model(model_path,\n                                     custom_objects={\"KerasLayer\" : hub.KerasLayer})\n  return model\n","metadata":{"execution":{"iopub.execute_input":"2022-06-26T10:20:30.940531Z","iopub.status.busy":"2022-06-26T10:20:30.940192Z","iopub.status.idle":"2022-06-26T10:20:30.949726Z","shell.execute_reply":"2022-06-26T10:20:30.948429Z","shell.execute_reply.started":"2022-06-26T10:20:30.940495Z"},"id":"5d119e6b-bdcb-4cb7-885a-a2101f5b06a7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save a model trained on 1000 images\nsave_model(model, suffix=\"1000-images-mobilenetv2-Adam\")","metadata":{"execution":{"iopub.execute_input":"2022-06-25T08:13:31.175865Z","iopub.status.busy":"2022-06-25T08:13:31.175596Z","iopub.status.idle":"2022-06-25T08:13:31.424941Z","shell.execute_reply":"2022-06-25T08:13:31.424163Z","shell.execute_reply.started":"2022-06-25T08:13:31.175840Z"},"id":"86dce0b8-b748-4bb0-a0c4-3620fb005b75","outputId":"c33c40e8-4f0b-4d72-d8c3-3eddc2ee6440"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load a model trained on 1000 images\nloaded_1000_image_model = load_model(\"Models/2022:06:25-08:071656144422-1000-images-mobilenetv2-Adam.h5\")","metadata":{"execution":{"iopub.execute_input":"2022-06-25T08:14:09.699251Z","iopub.status.busy":"2022-06-25T08:14:09.698953Z","iopub.status.idle":"2022-06-25T08:14:12.203897Z","shell.execute_reply":"2022-06-25T08:14:12.202957Z","shell.execute_reply.started":"2022-06-25T08:14:09.699224Z"},"id":"97bbaad7-0c28-402b-b35b-6b218999e73b","outputId":"9655fc81-b552-42b8-afac-3a4c3be715cc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_1000_image_model.evaluate(val_batch)","metadata":{"execution":{"iopub.execute_input":"2022-06-25T08:14:44.882624Z","iopub.status.busy":"2022-06-25T08:14:44.882334Z","iopub.status.idle":"2022-06-25T08:14:46.119395Z","shell.execute_reply":"2022-06-25T08:14:46.118800Z","shell.execute_reply.started":"2022-06-25T08:14:44.882596Z"},"id":"df094cb7-db8f-4dfe-9797-c2160fc25051","outputId":"81e5e8a3-78ef-4e24-b9be-7285b35ee1c1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NOW, lets train the mode with the full data","metadata":{"id":"d29b2214-42ae-456d-b163-11c4aa5bc250"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#Split to train and validation\nx_train, x_val, y_train, y_val = train_test_split(x, \n                                                  y,\n                                                  test_size=0.2,\n                                                  random_state=42)\nprint(len(x_train), len(y_train), len(x_val), len(y_val))\n\ntrain_batch = create_data_batches(x_train, y_train)\nval_batch = create_data_batches(x_val, y_val, valid_data=True)\ntrain_batch, val_batch","metadata":{"execution":{"iopub.execute_input":"2022-06-26T09:53:50.796215Z","iopub.status.busy":"2022-06-26T09:53:50.795870Z","iopub.status.idle":"2022-06-26T09:53:50.934408Z","shell.execute_reply":"2022-06-26T09:53:50.932988Z","shell.execute_reply.started":"2022-06-26T09:53:50.796177Z"},"id":"2211ba3a-ccba-4319-bb50-416045d74ad6","outputId":"5824b2e8-fded-4593-b3ba-9a1921f0d2b0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create full model\nfull_model = create_model()\n# Create full model callbacks\nfull_model_tensorboard = create_tensorboard_callback()\nfull_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3) # Stops when accuracy does not improve for 3 epochs","metadata":{"execution":{"iopub.execute_input":"2022-06-26T09:53:54.565454Z","iopub.status.busy":"2022-06-26T09:53:54.563867Z","iopub.status.idle":"2022-06-26T09:53:59.220189Z","shell.execute_reply":"2022-06-26T09:53:59.219154Z","shell.execute_reply.started":"2022-06-26T09:53:54.565390Z"},"id":"9be2de11-583a-4444-96f3-1727a8fedcf9","outputId":"625a39f0-b822-4f38-c73d-c54713accfdc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model to full data\nfull_model.fit(x=train_batch,\n                epochs=100,\n                validation_data=val_batch,\n                validation_freq=1,\n                callbacks=[full_model_tensorboard, full_model_early_stopping]\n                )","metadata":{"execution":{"iopub.execute_input":"2022-06-26T09:53:59.223277Z","iopub.status.busy":"2022-06-26T09:53:59.222767Z","iopub.status.idle":"2022-06-26T10:19:55.123180Z","shell.execute_reply":"2022-06-26T10:19:55.122029Z","shell.execute_reply.started":"2022-06-26T09:53:59.223231Z"},"id":"4cda5a90-9129-4c24-9e1e-1bfa4b71e555","outputId":"2b12d2d6-7531-46e6-e853-a028addd9a03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_model(full_model, suffix=\"full-image-set-mobilenetv2-Adam\")","metadata":{"execution":{"iopub.execute_input":"2022-06-26T10:20:44.010211Z","iopub.status.busy":"2022-06-26T10:20:44.009902Z","iopub.status.idle":"2022-06-26T10:20:44.583027Z","shell.execute_reply":"2022-06-26T10:20:44.582138Z","shell.execute_reply.started":"2022-06-26T10:20:44.010178Z"},"id":"4db7bfae-d6b1-4326-8274-3cdd74a41e36","outputId":"8559a751-25f5-4498-b946-6f43a1590ba1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing the loaded full model on some custom images","metadata":{"id":"f1027cd3-9ba2-4819-9772-4bbad0fb1604"}},{"cell_type":"code","source":"loaded_full_model = load_model(\"Models/2022:06:29-07:471656488859-full-image-set-mobilenetv2-Adam.h5\")","metadata":{"execution":{"iopub.execute_input":"2022-06-26T11:39:20.997517Z","iopub.status.busy":"2022-06-26T11:39:20.997289Z","iopub.status.idle":"2022-06-26T11:39:23.743323Z","shell.execute_reply":"2022-06-26T11:39:23.742341Z","shell.execute_reply.started":"2022-06-26T11:39:20.997500Z"},"id":"34c619b3-b9e5-4cc4-8e4c-4e20e1c12e3a","outputId":"a4638b30-4203-4650-8a89-6be8bbf49c89"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_images = [ \"Images/custom-images/\" + fname for fname in os.listdir(\"Images/custom-images\")]\ncustom_images","metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:17:15.188339Z","iopub.status.busy":"2022-06-26T12:17:15.187255Z","iopub.status.idle":"2022-06-26T12:17:15.219143Z","shell.execute_reply":"2022-06-26T12:17:15.217819Z","shell.execute_reply.started":"2022-06-26T12:17:15.188294Z"},"id":"66b437c9-a267-40f9-acd6-d161c59b9074","outputId":"0f5851c8-0f54-4a99-a2d6-ec8efb3f1378"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_batch = create_data_batches(custom_images, test_data=True)\ncustom_batch","metadata":{"execution":{"iopub.execute_input":"2022-06-26T11:42:51.107080Z","iopub.status.busy":"2022-06-26T11:42:51.106835Z","iopub.status.idle":"2022-06-26T11:42:51.124727Z","shell.execute_reply":"2022-06-26T11:42:51.124138Z","shell.execute_reply.started":"2022-06-26T11:42:51.107056Z"},"id":"d6e3bb7e-5c85-47d6-8f01-60f812666439","outputId":"f0fcab49-a9ed-4df4-8f15-bd4d09d103ce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_predictions = loaded_full_model.predict(custom_batch)\ncustom_predictions","metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:01:27.641898Z","iopub.status.busy":"2022-06-26T12:01:27.641613Z","iopub.status.idle":"2022-06-26T12:01:27.765037Z","shell.execute_reply":"2022-06-26T12:01:27.764159Z","shell.execute_reply.started":"2022-06-26T12:01:27.641874Z"},"id":"ec568bbf-699e-4585-8a76-1fbeb440dd0c","outputId":"307c366a-c179-45ff-d768-0e96d73c0d54"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's make a function to do ","metadata":{"id":"47c13999-72b5-4498-a8dd-c5cb5ff7c1d4"}},{"cell_type":"code","source":"# COMPLETE DOG BREED PREDICTION FUNCTION\ndef dog_breed_predict(filename_list: list, model):\n  # Turn images into batch datasets\n  file_batch = create_data_batches(filename_list, test_data=True)\n  # Make predictions on the data\n  prediction = model.predict(file_batch)\n  # Get custom image prediction labels\n  custom_pred_labels = [get_pred_label(i) for i in prediction]\n  # Unbatch batched datasets\n  custom_images = []\n  for image in file_batch.unbatch().as_numpy_iterator():\n    custom_images.append(image)\n  # Plot results\n  no_rows = len(filename_list)//3 + (0 if (len(filename_list)%3 == 0) else 1)\n  no_cols = 3\n  if len(filename_list) < 3:\n    no_cols = len(filename_list) * 3\n\n  plt.figure(figsize=(3*no_cols,3*no_rows))\n  for i, image in enumerate(custom_images):\n    # Confidence percentage\n    confidence_perc = np.max(prediction[i])*100\n\n    # Coloring based on confidence\n    if confidence_perc >= 85:\n        color = \"green\"\n    elif confidence_perc >= 70:\n        color = \"blue\"\n    elif confidence_perc >= 50:\n        color = \"orange\"\n    else:\n        color = \"red\"\n       \n    plt.subplot(no_rows,3,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(\"{} / {:2.2f}%\".format(custom_pred_labels[i], confidence_perc), color=color)\n    plt.imshow(image)","metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:11:20.236195Z","iopub.status.busy":"2022-06-26T12:11:20.235606Z","iopub.status.idle":"2022-06-26T12:11:20.244615Z","shell.execute_reply":"2022-06-26T12:11:20.243563Z","shell.execute_reply.started":"2022-06-26T12:11:20.236166Z"},"id":"b59646ef-dac8-4ac8-b0ff-8127948d60ec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dog_breed_predict(custom_images, loaded_full_model)","metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:17:24.241544Z","iopub.status.busy":"2022-06-26T12:17:24.241277Z","iopub.status.idle":"2022-06-26T12:17:25.150429Z","shell.execute_reply":"2022-06-26T12:17:25.149357Z","shell.execute_reply.started":"2022-06-26T12:17:24.241514Z"},"id":"63449a10-eb26-48bc-8d4a-f2f7504a6194","outputId":"59b76c4d-bc40-4a57-aece-10b6aadeafea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = custom_predictions[1].argsort()[-10:][::-1]\nfor i in idx:\n  print((unique_breeds[i] , f\"{custom_predictions[1][i]*100} %\"))","metadata":{"id":"_VQhfF3zUbqo","outputId":"fbd8c22f-b185-407f-a658-47d5934d8b54"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation for further improvement on model (Enrichment of training data)","metadata":{"id":"357c41eb-0d00-4494-93ae-676fdd136b11"}},{"cell_type":"code","source":"def augment_image(image, seed):\n  data_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\",seed=seed),\n    tf.keras.layers.RandomRotation(0.2,seed=seed),\n    tf.keras.layers.RandomZoom(0.2,seed=seed),\n    tf.keras.layers.RandomCrop(height=IMG_SIZE, width=IMG_SIZE,seed=seed),\n    tf.keras.layers.RandomContrast(0.25,seed=seed)\n  ])\n  return data_augmentation(image)\n  \n# Show example of augmentations\nimage = process_image(custom_images[1])\nimage = tf.cast(tf.expand_dims(image, 0), tf.float32)\n\nplt.figure(figsize=(10, 8))\nfor i in range(9):\n  augmented_image = augment_image(image, i)\n  ax = plt.subplot(3, 3, i + 1)\n  plt.imshow(augmented_image[0])\n  plt.axis(\"off\")","metadata":{"execution":{"iopub.execute_input":"2022-06-26T12:23:10.226534Z","iopub.status.busy":"2022-06-26T12:23:10.226257Z","iopub.status.idle":"2022-06-26T12:23:11.341815Z","shell.execute_reply":"2022-06-26T12:23:11.340808Z","shell.execute_reply.started":"2022-06-26T12:23:10.226508Z"},"id":"756eeaa7-d278-4b8b-8b49-c0cc4d7af3f8","outputId":"576ec489-9001-46cc-8c37-1427247949c7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets create a function that augments the data when creating a tuple (image, label)\ndef get_augmented_image_tuple(filename, label, seed=0):\n  \"\"\"\n  Takes an image file path and label\n  then processes & augments and return a tuple (image, label)\n  \"\"\"\n  image = process_image(filename)\n  return augment_image(image, seed), label","metadata":{"execution":{"iopub.status.busy":"2022-06-26T12:49:20.034841Z","iopub.status.idle":"2022-06-26T12:49:20.035308Z","shell.execute_reply":"2022-06-26T12:49:20.035083Z","shell.execute_reply.started":"2022-06-26T12:49:20.035054Z"},"id":"1fff2456-edea-4d22-94d8-dd2cebc3f2f7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets update the batching function\nBATCH_SIZE = 32\n\n#function to turn data to batches\ndef create_data_batches(x, y=None, batch_size=32, valid_data=False, test_data=False, augment=False, seed=0):\n  \"\"\"\n  Create batches of data out of (image x) and (label y) pairs.\n  Shuffles the data if its training data, but not when validation data.\n  Also accepts test data as input (no labels).\n  \"\"\"\n  #If test dataset, we probably don't have labels\n  if test_data:\n    print(f\"Creating test data batches... BATCH SIZE={batch_size}\")\n    data = tf.data.Dataset.from_tensor_slices(tf.constant(x))\n    data_batch = data.map(process_image).batch(batch_size)\n\n  #If validation dataset, we don't need to shuffle\n  elif valid_data:\n    print(f\"Creating validation data batches... BATCH SIZE={batch_size}\")\n    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), #filepath\n                                              tf.constant(y))) # labels\n    if augment:\n      print(f\"Augmenting seed = {seed}\")\n      data = data.map(lambda data, seed : get_augmented_image_tuple(data, seed))\n    else:\n      data = data.map(get_image_tuple)\n    data_batch = data.batch(batch_size)\n  \n  else:\n    print(f\"Creating training data batches... BATCH SIZE={batch_size}\")\n    #turn filepath and labels into Tensors\n    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), #filepath\n                                              tf.constant(y))) # labels\n    #shuffle\n    data = data.shuffle(buffer_size=len(x))\n\n    #process into (image,label) tuple and make batch\n    if augment:\n      print(f\"Augmenting seed = {seed}\")\n      data = data.map(lambda data, seed : get_augmented_image_tuple(data, seed))\n    else:\n      data = data.map(get_image_tuple)\n    data_batch = data.batch(batch_size)\n  return data_batch","metadata":{"id":"3laKkNU2XXEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create function for viewing images in data batches\n\ndef show_32_images(images, labels):\n  \"\"\"\n  Display a plot of 25 images and their labels from data batch\n  \"\"\"\n  plt.figure(figsize=(10,16))\n  for i in range(32):\n    #create subplots 4x8\n    ax = plt.subplot(8,4, i+1)\n    #Display Image\n    plt.imshow(images[i])\n    plt.title(unique_breeds[labels[i].argmax()])\n    #remove axis\n    plt.axis(\"off\")","metadata":{"id":"7935b67e-8f0a-4b08-a9dc-5fc54b73387a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## First Iteration of Data Augmentation Training\nAugment seed #0","metadata":{"id":"qJtGQrvzs3Dt"}},{"cell_type":"code","source":"train_augment_batch = create_data_batches(x_train, y_train, augment=True, seed=0)\nval_augment_batch = create_data_batches(x_val, y_val, valid_data=True, augment=True, seed=0)\ntrain_augment_batch, val_augment_batch","metadata":{"execution":{"iopub.status.busy":"2022-06-26T12:49:20.037800Z","iopub.status.idle":"2022-06-26T12:49:20.038300Z","shell.execute_reply":"2022-06-26T12:49:20.038081Z","shell.execute_reply.started":"2022-06-26T12:49:20.038055Z"},"id":"1b731140-d232-4c8a-9098-748a3e6f872e","outputId":"d20409a4-9973-440c-877e-c80bcc8cb783"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images, train_labels = next(train_augment_batch.as_numpy_iterator()) #This function unbatches it into a numpy array\nlen(train_images), len(train_labels) #shows shape 32,32 since its batch size is 32...","metadata":{"id":"CmmN0nqxYpfz","outputId":"2e528627-1d36-4cdf-a3aa-e760c8d5d7ec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_32_images(train_images, train_labels)","metadata":{"id":"PS_1kbOCYsJA","outputId":"ed851983-c322-4753-de4f-ca594533fc1d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now that we've augmented the dataset batches, let us incremently fit it into the previously trained model","metadata":{"id":"gRTIGGRYZKon"}},{"cell_type":"code","source":"# Create full model callbacks\nfull_augment_model_tensorboard = create_tensorboard_callback()\nfull_augment_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3) # Stops when accuracy does not improve for 3 epochs","metadata":{"id":"KyqNoHhDYwzy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model to full data\nloaded_full_model.fit(x=train_augment_batch,\n                epochs=100,\n                validation_data=val_augment_batch,\n                validation_freq=1,\n                callbacks=[full_augment_model_tensorboard, full_augment_model_early_stopping]\n                )","metadata":{"id":"A8UHeXu8ZXYM","outputId":"714334c6-7249-4894-8440-8e5713c15327"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_model(loaded_full_model, suffix=\"full-augment0-image-set-mobilenetv2-Adam\")","metadata":{"id":"NT5eciy7ZxKm","outputId":"22c2fb88-c3fa-4cf1-a069-8990ac7366dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_augment_model = load_model(\"Models/2022:06:29-08:541656492878-full-augment0-image-set-mobilenetv2-Adam.h5\")","metadata":{"id":"vN4jakzsZ9CF","outputId":"75551e8a-0d89-4d49-8d01-62474f94d595"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dog_breed_predict(custom_images, load_augment_model)","metadata":{"id":"EBb_ifjApPjp","outputId":"25f3d524-c076-4857-ed57-32e7ad74a36c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_augment_model.evaluate(val_batch)","metadata":{"id":"Xl2oBe5-paN9","outputId":"3defecb7-5750-4c4d-d9e6-32b571954ce7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_augment_model.evaluate(val_augment_batch)","metadata":{"id":"9lKgACxeqAVH","outputId":"4935c0d4-2a28-4c83-fde4-f04f05144b2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_full_model.evaluate(val_batch)","metadata":{"id":"si0SoYqJp5BT","outputId":"44d6d762-f85e-476c-d25e-1d6296b2ba2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_full_model.evaluate(val_augment_batch)","metadata":{"id":"PB6e4AktqVZI","outputId":"eeb57ca1-b55f-4045-db3f-37dd0f1c1b5c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation after 1st iteration of augmented data fitting\nAs seen above, the accuracy of the `load_augment_model` has a slightly worse accuracy than the `loaded_full_model` when evaluating the normal validation data batches. BUT it performs **WAY BETTER** than the latter when evaluating the augmented validation data batches.\n\n> This shows improvement in terms of the `model's overall scope` because it is generally performing better for all types of images (both normal and augmented)\n\n**Note:**\n Realistically, people do not always has perfect images of dogs, so our model needs to be able to predict images that may be slightly augmented (translated, rotated, brightness, contrast, cropped, etc)","metadata":{"id":"bnoYcPGSq7sm"}},{"cell_type":"markdown","source":"## Second Iteration of Data Augmentation Training\nAugment seed #1","metadata":{"id":"hTiWCq4UtDtO"}},{"cell_type":"code","source":"# THEN WE AUGMENT IT AGAIN...\ntrain_augment1_batch = create_data_batches(x_train, y_train, augment=True, seed=1)\nval_augment1_batch = create_data_batches(x_val, y_val, valid_data=True, augment=True, seed=1)\ntrain_augment1_batch, val_augment1_batch","metadata":{"id":"btc_UuAatsCd","outputId":"e17c0eb3-b583-418d-ad98-5c1889fdbfc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images, train_labels = next(train_augment1_batch.as_numpy_iterator()) #This function unbatches it into a numpy array\nshow_32_images(train_images, train_labels)","metadata":{"id":"tLXjOHX3uAW_","outputId":"d7e94a79-ce04-4355-ae88-1a415c5088b5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now that we've got our 2nd augmented data, lets fit it again","metadata":{"id":"BtW4tFNRwMXg"}},{"cell_type":"code","source":"load_augment_model = load_model(\"Models/2022_06_29-08_541656492878-full-augment0-image-set-mobilenetv2-Adam.h5\")","metadata":{"id":"llAKnFDTwPu2","outputId":"beb6fcdb-0459-4b85-bf75-8de70b2163a1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create full model callbacks\nfull_augment1_model_tensorboard = create_tensorboard_callback()\nfull_augment1_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3) # Stops when accuracy does not improve for 3 epochs","metadata":{"id":"Bb4WPoTOvrpx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model to full data\nload_augment_model.fit(x=train_augment1_batch,\n                epochs=100,\n                validation_data=val_augment1_batch,\n                validation_freq=1,\n                callbacks=[full_augment1_model_tensorboard, full_augment1_model_early_stopping]\n                )","metadata":{"id":"Q-Uy30B_yj7U","outputId":"954ffd5e-e5c3-4057-9eb6-93b4bc3e6f77"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"saved_path = save_model(load_augment_model, suffix=\"full-augment1-image-set-mobilenetv2-Adam\")","metadata":{"id":"PCNltyCoywHW","outputId":"7903b733-a7cf-4237-fc6f-d3b3928b67c7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.colab import files\nfiles.download(saved_path)","metadata":{"id":"6slPdvB04mra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augment1_model = load_model(\"Models/2022:06:30-05:151656566150-full-augment1-image-set-mobilenetv2-Adam.h5\")","metadata":{"id":"1sxrKMyd1ya8","outputId":"36758cd6-d5c5-4608-921e-8b3f5547e41d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dog_breed_predict(custom_images, augment1_model)","metadata":{"id":"vF8Ea7JX12ZN","outputId":"ff1fc32b-af35-4cae-e1bf-f6cd75d557c6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model_before, model_after, normal_batch, augment_batch):\n  \"\"\"\n  Evaluates model and compare it with the previous model\n  \"\"\"\n  print(\"NORMAL BATCH EVALUATION\")\n  print(\"Previous Model:\")\n  model_before.evaluate(normal_batch)\n  print(\"New Model:\")\n  model_after.evaluate(normal_batch)\n  print(\"\\nAUGMENTED BATCH EVALUATION\")\n  print(\"Previous Model:\")\n  model_before.evaluate(augment_batch)\n  print(\"New Model:\")\n  model_after.evaluate(augment_batch)","metadata":{"id":"sVBwy46Q1-d3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(load_augment_model, augment1_model, val_batch, val_augment1_batch)","metadata":{"id":"_62BBLq527LT","outputId":"ef661d0a-5640-40b3-da64-60170f1a3f28"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation after 2nd iteration of augmented data fitting\nAs seen above, the model's accuracy on normal batch is yet again decreasing, but this time it shows worse accuracy on the augmented batch.\nFor the reason stated, we will be continuing with the previous model","metadata":{"id":"wXitG00S3b0E"}},{"cell_type":"markdown","source":"## Third Iteration of Data Augmentation Training\nAugment seed #2","metadata":{"id":"y0xG34cI3cbJ"}},{"cell_type":"code","source":"train_augment2_batch = create_data_batches(x_train, y_train, augment=True, seed=2)\nval_augment2_batch = create_data_batches(x_val, y_val, valid_data=True, augment=True, seed=2)\ntrain_augment2_batch, val_augment_batch","metadata":{"id":"-2WKYWMw3SSZ","outputId":"d03029eb-8539-43a6-c1b6-21ccd6a2f063"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images, train_labels = next(train_augment2_batch.as_numpy_iterator()) #This function unbatches it into a numpy array\nlen(train_images), len(train_labels) #shows shape 32,32 since its batch size is 32...","metadata":{"id":"RiJVxWKID2kY","outputId":"831a6012-361f-44b3-b714-75ae5f37b070"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_32_images(train_images, train_labels)","metadata":{"id":"AUTf7hviD5BP","outputId":"6a2a0f3b-fe5e-4b6f-ef28-b9de8db6746e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Augmented Data Fitting","metadata":{"id":"di248-5rFUeZ"}},{"cell_type":"code","source":"loaded_model = load_model(\"Models/2022_06_29-08_541656492878-full-augment0-image-set-mobilenetv2-Adam.h5\")","metadata":{"id":"uPy26N6pFYz4","outputId":"dd22dad6-51de-4478-bca8-ef2914dcd610"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create full model callbacks\nfull_augment2_model_tensorboard = create_tensorboard_callback()\nfull_augment2_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3) # Stops when accuracy does not improve for 3 epochs","metadata":{"id":"eJTNt1upFLNd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model to full data\nloaded_model.fit(x=train_augment2_batch,\n                epochs=100,\n                validation_data=val_augment2_batch,\n                validation_freq=1,\n                callbacks=[full_augment2_model_tensorboard, full_augment2_model_early_stopping]\n                )","metadata":{"id":"iYBChw1-FlBd","outputId":"ece887b8-c145-45bc-df91-e7c9302155dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"saved_path = save_model(loaded_model, suffix=\"full-augment2-image-set-mobilenetv2-Adam\")","metadata":{"id":"rh_9SQM0FtjS","outputId":"41055fd3-469d-4cdf-fc76-72360d818d1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_augment2 = load_model(saved_path)","metadata":{"id":"1PN6TDycGJNW","outputId":"5f9263be-b56e-4239-d6f3-e6e6a875e82a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dog_breed_predict(custom_images, loaded_augment2)","metadata":{"id":"jKNhjYkdGZJV","outputId":"6935c7b7-e7ec-4711-e7c0-8eb8d92ce43f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prev_model = load_model(\"/content/Models/2022_06_29-08_541656492878-full-augment0-image-set-mobilenetv2-Adam.h5\")","metadata":{"id":"qyPN21leQCUC","outputId":"660d3950-de7a-432b-848c-a2c3cd06fcd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(prev_model, loaded_augment2, val_batch, val_augment2_batch)","metadata":{"id":"vRlthA-bQNU_","outputId":"c021aa3e-bb10-48ac-a1d0-4726764b3c2a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation after 3rd iteration of augmented data fitting\nAs seen above, the model's accuracy on normal batch is yet again decreasing, but this time it shows worse accuracy on the augmented batch.\nFor the reason stated, we will be continuing with the previous model","metadata":{"id":"K9Klk7GrRr3I"}},{"cell_type":"markdown","source":"## 3rd Iteration of Augmented Data Training\nAugment seed #3","metadata":{"id":"WslZedH-WOI_"}},{"cell_type":"code","source":"def augment_training(prev_model_path, seed, x_train=x_train, y_train=y_train, x_val=x_val, y_val=y_val, custom_images=custom_images, val_batch=val_batch):\n  \"\"\"\n  Function for the entire Iteration of Augmented data training\n  \"\"\"\n  # Augmentation\n  train_augment_batch = create_data_batches(x_train, y_train, augment=True, seed=seed)\n  val_augment_batch = create_data_batches(x_val, y_val, valid_data=True, augment=True, seed=seed)\n\n  # Training prep\n  model = load_model(prev_model_path)\n  full_augment_model_tensorboard = create_tensorboard_callback()\n  full_augment_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3) # Stops when accuracy does not improve for 3 epochs\n  \n  # Fit the model to full data\n  loaded_model.fit(x=train_augment_batch,\n                epochs=100,\n                validation_data=val_augment_batch,\n                validation_freq=1,\n                callbacks=[full_augment_model_tensorboard, full_augment_model_early_stopping]\n                )\n  saved_path = save_model(loaded_model, suffix=f\"full-augment{seed}-image-set-mobilenetv2-Adam\")\n  \n  # Evaluation\n  prev_model = load_model(prev_model_path)\n  dog_breed_predict(custom_images, loaded_model)\n  evaluate_model(prev_model, loaded_model, val_batch, val_augment_batch)","metadata":{"id":"6uUbgI2PQV-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augment_training(\"Models/2022_06_29-08_541656492878-full-augment0-image-set-mobilenetv2-Adam.h5\", 3)","metadata":{"id":"HG6fIAIhSFqI","outputId":"35574cca-4f5a-44c0-a896-96b984e2e355"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augment_training(\"Models/2022:06:30-07:121656573132-full-augment3-image-set-mobilenetv2-Adam.h5\", 4)","metadata":{"id":"J2J2PRI2WTi3","outputId":"e86c6259-b264-4540-ef36-0e85c97f3ed3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r Logs.zip Logs","metadata":{"id":"7S5F7IUzgDAe","outputId":"fdc9d719-47a7-4358-9fef-12d7cd4adb9d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augment_training(\"/content/Models/2022:06:30-07:451656575156-full-augment4-image-set-mobilenetv2-Adam.h5\", 5)","metadata":{"id":"kDFkZn3IlYJR","outputId":"ee017bf2-6963-4ad0-af56-583f17ba5d7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augment_training(\"/content/Models/2022:06:30-08:221656577369-full-augment5-image-set-mobilenetv2-Adam.h5\", 6)","metadata":{"id":"VZTTGrzZngc8","outputId":"fa064631-53b7-45c8-b85e-ba8058cd416e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augment_training(\"/content/Models/2022_06_30-08_48_1656578928-full-augment6-image-set-mobilenetv2-Adam.h5\", 7)","metadata":{"id":"oifyYkQrupM8","outputId":"fde1f268-db9f-4be6-ac74-9b8fcd354897"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augment_training(\"Models/2022_06_30-09_27_1656581275-full-augment7-image-set-mobilenetv2-Adam.h5\", 8)","metadata":{"id":"KyExGozn0L0u","outputId":"ec3b9939-bf7e-4747-cfe1-0fcd6768a663"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r Models.zip Models","metadata":{"id":"LLGG40eV9PYc","outputId":"c1562d66-856e-4419-98bc-f88399c87cb8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r Logs.zip Logs","metadata":{"id":"EyKEtsKN_lZG","outputId":"f83ee42f-b56c-4df6-ec12-1c7968a0bd1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augment_training(\"Models/2022_06_30-09_54_1656582840-full-augment8-image-set-mobilenetv2-Adam.h5\", 9)","metadata":{"id":"cIt-SKcB_4xE","outputId":"ac8f108b-c392-4680-c539-1c1f709f213a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"vuPTMcG0DP5E"},"execution_count":null,"outputs":[]}]}